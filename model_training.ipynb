{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9359f6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    f1_score, roc_auc_score, average_precision_score, precision_recall_curve,\n",
    "    accuracy_score, classification_report\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f43e5d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------- helpers -------------------\n",
    "def purged_splits(n_samples, n_splits=5, embargo=50):\n",
    "    fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=int)\n",
    "    fold_sizes[: n_samples % n_splits] += 1\n",
    "    starts = np.cumsum(fold_sizes) - fold_sizes\n",
    "    for i in range(n_splits):\n",
    "        test_start = starts[i]\n",
    "        test_end   = test_start + fold_sizes[i]\n",
    "        train_end  = max(0, test_start - embargo)\n",
    "        train_idx  = np.arange(0, train_end)\n",
    "        test_idx   = np.arange(test_start, test_end)\n",
    "        if len(train_idx) == 0:\n",
    "            continue\n",
    "        yield train_idx, test_idx\n",
    "\n",
    "def coerce_numeric_df(X):\n",
    "    X = X.copy()\n",
    "    for c in X.columns:\n",
    "        if not np.issubdtype(X[c].dtype, np.number):\n",
    "            X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "    return X.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab3093d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/clean_dataset.csv')\n",
    "df.columns\n",
    "\n",
    "# ------------------- 1) build 3 targets (no leakage) -------------------\n",
    "df = df.copy()\n",
    "\n",
    "H = 20  # lookahead horizon\n",
    "coin_specs = {\n",
    "    \"btc\": \"bitcoin_return\",\n",
    "    \"eth\": \"ethereum_return\",\n",
    "    \"ltc\": \"litecoin_return\",\n",
    "}\n",
    "\n",
    "for c, ret_col in coin_specs.items():\n",
    "    fut = df[ret_col].shift(-H).rolling(H).sum()\n",
    "    df[f\"y_{c}\"] = (fut > 0).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2091d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- 2) make lagged features -------------------\n",
    "# Use all columns except the labels as features, then lag by +1 so features are strictly past\n",
    "label_cols = [f\"y_{c}\" for c in coin_specs.keys()]\n",
    "X_lagged = df.drop(columns=label_cols, errors=\"ignore\").shift(1)\n",
    "\n",
    "# Build final modeling frame and drop NaNs from rolling/shift\n",
    "data = pd.concat([X_lagged, df[label_cols]], axis=1).dropna().reset_index(drop=True)\n",
    "X = coerce_numeric_df(data.drop(columns=label_cols))\n",
    "\n",
    "y_dict = {c: data[f\"y_{c}\"].astype(int).values for c in coin_specs.keys()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca1cce6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/Helia/lib/python3.13/site-packages/xgboost/callback.py:264: UserWarning: [11:29:05] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1758007651359/work/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/opt/miniconda3/envs/Helia/lib/python3.13/site-packages/xgboost/callback.py:264: UserWarning: [11:29:06] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1758007651359/work/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'btc': {'accuracy': 0.6107091172214182, 'roc_auc': 0.6223296938048897, 'avg_precision': 0.5114024987231941, 'report': '              precision    recall  f1-score   support\\n\\n           0      0.771     0.423     0.546       383\\n           1      0.541     0.844     0.659       308\\n\\n    accuracy                          0.611       691\\n   macro avg      0.656     0.634     0.603       691\\nweighted avg      0.669     0.611     0.597       691\\n'}, 'eth': {'accuracy': 0.6208393632416788, 'roc_auc': 0.6751154635648307, 'avg_precision': 0.5566833968826717, 'report': '              precision    recall  f1-score   support\\n\\n           0      0.812     0.438     0.569       395\\n           1      0.536     0.865     0.661       296\\n\\n    accuracy                          0.621       691\\n   macro avg      0.674     0.651     0.615       691\\nweighted avg      0.694     0.621     0.609       691\\n'}, 'ltc': {'accuracy': 0.5803183791606368, 'roc_auc': 0.5568874338957441, 'avg_precision': 0.5708292528735728, 'report': '              precision    recall  f1-score   support\\n\\n           0      0.615     0.324     0.425       330\\n           1      0.569     0.814     0.670       361\\n\\n    accuracy                          0.580       691\\n   macro avg      0.592     0.569     0.547       691\\nweighted avg      0.591     0.580     0.553       691\\n'}}\n"
     ]
    }
   ],
   "source": [
    "# ------------------- 3) train 3 models with walk-forward + embargo -------------------\n",
    "results = {}\n",
    "models  = {}\n",
    "thresholds = {}\n",
    "\n",
    "for coin in [\"btc\", \"eth\", \"ltc\"]:\n",
    "    y = y_dict[coin]\n",
    "    n = len(X)\n",
    "\n",
    "    oof_proba = np.zeros(n, dtype=float)\n",
    "    oof_pred  = np.zeros(n, dtype=int)\n",
    "\n",
    "    for tr_idx, te_idx in purged_splits(n, n_splits=5, embargo=50):\n",
    "        # validation is the tail of the train slice\n",
    "        val_portion = max(1, int(0.2 * len(tr_idx)))\n",
    "        if len(tr_idx) <= val_portion:\n",
    "            continue\n",
    "        val_idx = tr_idx[-val_portion:]\n",
    "        tr_core = tr_idx[:-val_portion]\n",
    "\n",
    "        X_tr, y_tr = X.iloc[tr_core], y[tr_core]\n",
    "        X_val, y_val = X.iloc[val_idx], y[val_idx]\n",
    "        X_te,  y_te  = X.iloc[te_idx],  y[te_idx]\n",
    "\n",
    "        pos = (y_tr == 1).sum(); neg = (y_tr == 0).sum()\n",
    "        spw = max(1.0, neg / max(1, pos))\n",
    "\n",
    "        clf = XGBClassifier(\n",
    "            tree_method=\"hist\",\n",
    "            max_depth=6,\n",
    "            learning_rate=0.05,\n",
    "            n_estimators=2000,   # early stop governs\n",
    "            subsample=0.9,\n",
    "            colsample_bytree=0.9,\n",
    "            min_child_weight=3,\n",
    "            gamma=1.0,\n",
    "            reg_lambda=2.0,\n",
    "            scale_pos_weight=spw,\n",
    "            eval_metric=\"auc\",\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        clf.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_tr, y_tr), (X_val, y_val)],\n",
    "            verbose=False\n",
    "            # early_stopping_rounds=100\n",
    "        )\n",
    "\n",
    "        # choose threshold on the validation tail (skip if single-class val)\n",
    "        proba_val = clf.predict_proba(X_val)[:, 1]\n",
    "        best_t = 0.5\n",
    "        if y_val.min() != y_val.max():\n",
    "            prec, rec, th = precision_recall_curve(y_val, proba_val)\n",
    "            best_f1 = -1.0\n",
    "            for t in th:\n",
    "                f1 = f1_score(y_val, (proba_val >= t).astype(int))\n",
    "                if f1 > best_f1:\n",
    "                    best_f1, best_t = f1, t\n",
    "\n",
    "        proba_te = clf.predict_proba(X_te)[:, 1]\n",
    "        oof_proba[te_idx] = proba_te\n",
    "        oof_pred[te_idx]  = (proba_te >= best_t).astype(int)\n",
    "    # choose a single global threshold from OOF predictions\n",
    "    # make sure oof_proba has been filled for all test folds\n",
    "    try:\n",
    "        prec, rec, th = precision_recall_curve(y, oof_proba)\n",
    "        # default if degenerate\n",
    "        best_t = 0.5\n",
    "        best_f1 = -1.0\n",
    "        for t in th:\n",
    "            f1 = f1_score(y, (oof_proba >= t).astype(int))\n",
    "            if f1 > best_f1:\n",
    "                best_f1, best_t = f1, t\n",
    "    except Exception:\n",
    "        best_t = 0.5\n",
    "\n",
    "    # fit one final model per coin on ALL data\n",
    "    # you can keep the same hyperparams; optionally do early stopping on a tail split\n",
    "    val_portion = max(1, int(0.2 * len(X)))\n",
    "    val_idx = np.arange(len(X) - val_portion, len(X))\n",
    "    tr_idx = np.arange(0, len(X) - val_portion)\n",
    "\n",
    "    X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n",
    "    X_val, y_val = X.iloc[val_idx], y[val_idx]\n",
    "\n",
    "    pos = (y_tr == 1).sum(); neg = (y_tr == 0).sum()\n",
    "    spw = max(1.0, neg / max(1, pos))\n",
    "\n",
    "    final_clf = XGBClassifier(\n",
    "        tree_method=\"hist\",\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=2000,     # use early stopping\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        min_child_weight=3,\n",
    "        gamma=1.0,\n",
    "        reg_lambda=2.0,\n",
    "        scale_pos_weight=spw,\n",
    "        eval_metric=\"auc\",\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    final_clf.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_tr, y_tr), (X_val, y_val)],\n",
    "        verbose=False\n",
    "        # early_stopping_rounds=100\n",
    "    )\n",
    "\n",
    "    models[coin] = final_clf\n",
    "    thresholds[coin] = float(best_t)\n",
    "\n",
    "    # store per-coin metrics (you already had this)\n",
    "    acc = accuracy_score(y, oof_pred)\n",
    "    auc = roc_auc_score(y, oof_proba)\n",
    "    ap  = average_precision_score(y, oof_proba)\n",
    "    rep = classification_report(y, oof_pred, digits=3)\n",
    "\n",
    "    results[coin] = {\"accuracy\": acc, \"roc_auc\": auc, \"avg_precision\": ap, \"report\": rep}\n",
    "\n",
    "print(results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c8750a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/training_results.joblib']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib, os\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "joblib.dump(models[\"btc\"], \"models/btc_xgb.joblib\")\n",
    "joblib.dump(models[\"eth\"], \"models/eth_xgb.joblib\")\n",
    "joblib.dump(models[\"ltc\"], \"models/ltc_xgb.joblib\")\n",
    "\n",
    "joblib.dump(list(X.columns), \"models/feature_columns.joblib\")\n",
    "joblib.dump(thresholds, \"models/thresholds.joblib\")\n",
    "joblib.dump(results, \"models/training_results.joblib\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Helia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
